from fastapi import FastAPI, Request, Query, UploadFile, File, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import json
import os
import shutil
import re
from collections import defaultdict, Counter
from datetime import datetime
import statistics
from pathlib import Path
import uuid

app = FastAPI()

app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# è¨­å®šè³‡æ–™ç›®éŒ„
DATA_DIR = Path("analysis_data")
DATA_DIR.mkdir(exist_ok=True)

# å…¨åŸŸè®Šæ•¸å„²å­˜ç•¶å‰é¸æ“‡çš„åˆ†ææª”æ¡ˆ
current_analysis = {
    "name": "é è¨­åˆ†æ",
    "summary_data": [],
    "template_to_raw_dict": {},
    "raw_data": []
}

def load_analysis_data(analysis_name="é è¨­åˆ†æ"):
    """è¼‰å…¥æŒ‡å®šçš„åˆ†æè³‡æ–™"""
    global current_analysis
    
    if analysis_name == "é è¨­åˆ†æ":
        # è¼‰å…¥é è¨­è³‡æ–™
        try:
            with open("normalized_sql_summary.json", "r", encoding="utf-8") as f:
                summary_data = json.load(f)
            with open("parsed_slow_log.json", "r", encoding="utf-8") as f:
                raw_data = json.load(f)
        except FileNotFoundError:
            summary_data = []
            raw_data = []
    else:
        # è¼‰å…¥æŒ‡å®šåˆ†ææª”æ¡ˆ
        analysis_path = DATA_DIR / analysis_name
        try:
            with open(analysis_path / "summary.json", "r", encoding="utf-8") as f:
                summary_data = json.load(f)
            with open(analysis_path / "raw_data.json", "r", encoding="utf-8") as f:
                raw_data = json.load(f)
        except FileNotFoundError:
            raise HTTPException(status_code=404, detail="åˆ†ææª”æ¡ˆä¸å­˜åœ¨")
    
    # å»ºç«‹æ¨£æ¿å°æ‡‰é—œä¿‚
    template_to_raw = defaultdict(list)
    for item in raw_data:
        if item.get("sql"):
            normalized = normalize_sql(item["sql"])
            template_to_raw[normalized].append({
                "original_sql": item["sql"],
                "query_time": item.get("query_time", 0),
                "time": item.get("time", ""),
                "user": item.get("user", ""),
                "host": item.get("host", ""),
                "rows_examined": item.get("rows_examined", 0),
                "rows_sent": item.get("rows_sent", 0),
                "lock_time": item.get("lock_time", 0),
                "timestamp": item.get("timestamp", 0),
                "thread_id": item.get("thread_id", 0),
                "schema": item.get("schema", ""),
                "tables_used": item.get("tables_used", [])
            })
    
    current_analysis = {
        "name": analysis_name,
        "summary_data": summary_data,
        "template_to_raw_dict": dict(template_to_raw),
        "raw_data": raw_data
    }

def normalize_sql(sql: str) -> str:
    """SQLæ¨£æ¿è½‰æ›å‡½å¼"""
    sql = sql.lower()
    sql = re.sub(r"\s+", " ", sql)
    sql = re.sub(r"'[^']*'", "?", sql)
    sql = re.sub(r'"[^"]*"', "?", sql)
    sql = re.sub(r"\b\d+\.\d+\b", "?", sql)
    sql = re.sub(r"\b\d+\b", "?", sql)
    sql = re.sub(r"\(\s*(\?,\s*)*\?\s*\)", "(?)", sql)
    return sql.strip()

def get_sql_type(sql: str) -> str:
    """SQLé¡å‹åˆ¤æ–·"""
    match = re.match(r"^\s*(\w+)", sql.lower())
    if not match:
        return "OTHER"
    keyword = match.group(1).upper()
    if keyword in {"SELECT", "INSERT", "UPDATE", "DELETE", "REPLACE", "CALL"}:
        return keyword
    return "OTHER"

def parse_slow_log(content: str) -> list:
    """è§£ææ…¢æŸ¥è©¢LOGå…§å®¹"""
    entries = re.split(r"(?=^# Time: )", content, flags=re.MULTILINE)
    parsed_entries = []
    
    table_pattern = re.compile(r"(?:from|join)\s+`?(\w+)`?(?:\s+as|\s+\w+)?", re.IGNORECASE)
    
    for entry in entries:
        if not entry.strip():
            continue
            
        parsed = {
            "time": None, "user": None, "host": None, "thread_id": None,
            "schema": None, "qc_hit": None, "query_time": None, "lock_time": None,
            "rows_sent": None, "rows_examined": None, "rows_affected": None,
            "bytes_sent": None, "timestamp": None, "sql": None, "tables_used": []
        }
        
        # è§£æå„å€‹æ¬„ä½
        if m := re.search(r"# Time: (.+)", entry):
            parsed["time"] = m.group(1).strip()
        
        if m := re.search(r"# User@Host: (.+)\[(.+)\] @  \[(.*)\]", entry):
            parsed["user"] = m.group(2).strip()
            parsed["host"] = m.group(3).strip()
        
        if m := re.search(r"# Thread_id: (\d+)\s+Schema: (\w+)\s+QC_hit: (\w+)", entry):
            parsed["thread_id"] = int(m.group(1))
            parsed["schema"] = m.group(2)
            parsed["qc_hit"] = m.group(3)
        
        if m := re.search(r"# Query_time: ([\d.]+)\s+Lock_time: ([\d.]+)\s+Rows_sent: (\d+)\s+Rows_examined: (\d+)", entry):
            parsed["query_time"] = float(m.group(1))
            parsed["lock_time"] = float(m.group(2))
            parsed["rows_sent"] = int(m.group(3))
            parsed["rows_examined"] = int(m.group(4))
        
        if m := re.search(r"# Rows_affected: (\d+)\s+Bytes_sent: (\d+)", entry):
            parsed["rows_affected"] = int(m.group(1))
            parsed["bytes_sent"] = int(m.group(2))
        
        if m := re.search(r"SET timestamp=(\d+);", entry):
            parsed["timestamp"] = int(m.group(1))
        
        if m := re.search(r"SET timestamp=\d+;\n(.+)", entry, re.DOTALL):
            sql = m.group(1).strip()
            parsed["sql"] = sql
            tables = table_pattern.findall(sql)
            parsed["tables_used"] = sorted(set(tables))
        
        parsed_entries.append(parsed)
    
    return parsed_entries

def create_summary_data(raw_data: list) -> list:
    """å»ºç«‹çµ±è¨ˆæ‘˜è¦è³‡æ–™"""
    sql_groups = defaultdict(list)
    
    for item in raw_data:
        sql = item.get("sql")
        if sql:
            norm_sql = normalize_sql(sql)
            sql_groups[norm_sql].append(item)
    
    summary = []
    for norm_sql, entries in sql_groups.items():
        count = len(entries)
        total_time = sum(e["query_time"] for e in entries if e.get("query_time") is not None)
        avg_time = total_time / count if count else 0
        sql_type = get_sql_type(norm_sql)
        
        # æ”¶é›†æ‰€æœ‰æ¶‰åŠçš„è¡¨æ ¼
        all_tables = set()
        for entry in entries:
            all_tables.update(entry.get("tables_used", []))
        
        summary.append({
            "template": norm_sql,
            "type": sql_type,
            "count": count,
            "avg_query_time": round(avg_time, 4),
            "tables_used": sorted(list(all_tables))
        })
    
    return summary

# åˆå§‹åŒ–è¼‰å…¥é è¨­è³‡æ–™
try:
    load_analysis_data()
except Exception as e:
    print(f"âš ï¸ ç„¡æ³•è¼‰å…¥é è¨­è³‡æ–™: {e}")

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    # ç¢ºä¿è³‡æ–™å­˜åœ¨
    if not current_analysis["summary_data"] or len(current_analysis["raw_data"]) == 0:
        # å¦‚æœæ²’æœ‰è³‡æ–™ï¼Œå˜—è©¦è¼‰å…¥ç¬¬ä¸€å€‹å¯ç”¨çš„åˆ†ææª”æ¡ˆ
        if DATA_DIR.exists():
            for item in DATA_DIR.iterdir():
                if item.is_dir() and (item / "summary.json").exists():
                    try:
                        load_analysis_data(item.name)
                        print(f"è‡ªå‹•è¼‰å…¥åˆ†ææª”æ¡ˆ: {item.name}")
                        break
                    except:
                        continue
    
    return templates.TemplateResponse("index.html", {
        "request": request, 
        "data": current_analysis["summary_data"],
        "template_to_raw": current_analysis["template_to_raw_dict"],
        "current_analysis": current_analysis["name"]
    })

@app.post("/api/upload_log")
async def upload_log(
    file: UploadFile = File(...),
    analysis_name: str = Form(...)
):
    """ä¸Šå‚³ä¸¦åˆ†æLOGæª”æ¡ˆ"""
    
    # æª¢æŸ¥æª”æ¡ˆé¡å‹
    if not file.filename or not file.filename.endswith(('.log', '.txt')):
        raise HTTPException(status_code=400, detail="åªæ”¯æ´ .log æˆ– .txt æª”æ¡ˆ")
    
    # å»ºç«‹åˆ†æç›®éŒ„
    analysis_path = DATA_DIR / analysis_name
    analysis_path.mkdir(exist_ok=True)
    
    try:
        # å„²å­˜åŸå§‹æª”æ¡ˆ
        log_file_path = analysis_path / f"original_{file.filename}"
        with open(log_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # è®€å–ä¸¦è§£æLOGå…§å®¹
        with open(log_file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()
        
        # è§£æLOG
        raw_data = parse_slow_log(content)
        
        # å»ºç«‹çµ±è¨ˆæ‘˜è¦
        summary_data = create_summary_data(raw_data)
        
        # å„²å­˜åˆ†æçµæœ
        with open(analysis_path / "raw_data.json", "w", encoding="utf-8") as f:
            json.dump(raw_data, f, ensure_ascii=False, indent=2)
        
        with open(analysis_path / "summary.json", "w", encoding="utf-8") as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜å…ƒè³‡æ–™
        metadata = {
            "original_filename": file.filename,
            "upload_time": datetime.now().isoformat(),
            "total_queries": len(raw_data),
            "total_templates": len(summary_data)
        }
        with open(analysis_path / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "message": f"LOGæª”æ¡ˆ '{file.filename}' ä¸Šå‚³ä¸¦åˆ†æå®Œæˆ",
            "analysis_name": analysis_name,
            "total_queries": len(raw_data),
            "total_templates": len(summary_data)
        }
        
    except Exception as e:
        # æ¸…ç†å¤±æ•—çš„ç›®éŒ„
        if analysis_path.exists():
            shutil.rmtree(analysis_path)
        raise HTTPException(status_code=500, detail=f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

@app.post("/api/switch_analysis/{analysis_name}")
async def switch_analysis(analysis_name: str):
    """åˆ‡æ›åˆ°æŒ‡å®šçš„åˆ†ææª”æ¡ˆ"""
    try:
        load_analysis_data(analysis_name)
        return {
            "success": True,
            "message": f"å·²åˆ‡æ›åˆ°åˆ†ææª”æ¡ˆ: {analysis_name}",
            "current_analysis": current_analysis["name"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ‡æ›åˆ†ææª”æ¡ˆå¤±æ•—: {str(e)}")

@app.get("/api/analysis_files")
async def get_analysis_files():
    """å–å¾—å¯ç”¨çš„åˆ†ææª”æ¡ˆåˆ—è¡¨"""
    analysis_files = []
    
    # æª¢æŸ¥æ˜¯å¦æœ‰çœŸæ­£çš„é è¨­åˆ†ææª”æ¡ˆï¼ˆparsed_slow_log.json å’Œ normalized_sql_summary.jsonï¼‰
    has_default_files = (
        os.path.exists("parsed_slow_log.json") and 
        os.path.exists("normalized_sql_summary.json")
    )
    
    # åªæœ‰ç•¶é è¨­åˆ†ææª”æ¡ˆå­˜åœ¨ä¸”æœ‰è³‡æ–™æ™‚æ‰é¡¯ç¤º
    if has_default_files and current_analysis["name"] == "é è¨­åˆ†æ" and len(current_analysis["raw_data"]) > 0:
        analysis_files.append({
            "name": "é è¨­åˆ†æ",
            "is_current": True,
            "metadata": {
                "total_queries": len(current_analysis["raw_data"]),
                "total_templates": len(current_analysis["summary_data"]),
                "upload_time": "å…§å»ºè³‡æ–™"
            }
        })
    
    # ç”¨æˆ¶ä¸Šå‚³çš„åˆ†ææª”æ¡ˆ
    if DATA_DIR.exists():
        for item in DATA_DIR.iterdir():
            if item.is_dir() and (item / "summary.json").exists():
                try:
                    with open(item / "metadata.json", "r", encoding="utf-8") as f:
                        metadata = json.load(f)
                except:
                    metadata = {"upload_time": "æœªçŸ¥", "total_queries": 0, "total_templates": 0}
                
                analysis_files.append({
                    "name": item.name,
                    "is_current": current_analysis["name"] == item.name,
                    "metadata": metadata
                })
    
    return {"analysis_files": analysis_files}

@app.delete("/api/analysis_files/{analysis_name}")
async def delete_analysis(analysis_name: str):
    """åˆªé™¤æŒ‡å®šçš„åˆ†ææª”æ¡ˆ"""
    if analysis_name == "é è¨­åˆ†æ":
        raise HTTPException(status_code=400, detail="ç„¡æ³•åˆªé™¤é è¨­åˆ†æ")
    
    analysis_path = DATA_DIR / analysis_name
    if not analysis_path.exists():
        raise HTTPException(status_code=404, detail="åˆ†ææª”æ¡ˆä¸å­˜åœ¨")
    
    try:
        shutil.rmtree(analysis_path)
        
        # å¦‚æœåˆªé™¤çš„æ˜¯ç•¶å‰åˆ†æï¼Œåˆ‡æ›å›é è¨­
        if current_analysis["name"] == analysis_name:
            load_analysis_data("é è¨­åˆ†æ")
        
        return {
            "success": True,
            "message": f"åˆ†ææª”æ¡ˆ '{analysis_name}' å·²åˆªé™¤",
            "current_analysis": current_analysis["name"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆªé™¤å¤±æ•—: {str(e)}")

@app.post("/api/merge_analysis")
async def merge_analysis(request: Request):
    """åˆä½µå¤šå€‹åˆ†ææª”æ¡ˆ"""
    try:
        body = await request.json()
        merged_name = body.get("merged_name")
        source_files = body.get("source_files", [])
        
        if not merged_name:
            raise HTTPException(status_code=400, detail="è«‹æä¾›åˆä½µæª”æ¡ˆåç¨±")
        
        if len(source_files) < 2:
            raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦é¸æ“‡ 2 å€‹æª”æ¡ˆé€²è¡Œåˆä½µ")
        
        # æª¢æŸ¥åˆä½µæª”æ¡ˆåç¨±æ˜¯å¦å·²å­˜åœ¨
        merged_path = DATA_DIR / merged_name
        if merged_path.exists():
            raise HTTPException(status_code=400, detail="åˆä½µæª”æ¡ˆåç¨±å·²å­˜åœ¨ï¼Œè«‹é¸æ“‡å…¶ä»–åç¨±")
        
        # æ”¶é›†æ‰€æœ‰åŸå§‹è³‡æ–™
        all_raw_data = []
        source_metadata = []
        
        for source_name in source_files:
            if source_name == "é è¨­åˆ†æ":
                # å¦‚æœæ˜¯é è¨­åˆ†æï¼Œä½¿ç”¨ç•¶å‰è¼‰å…¥çš„è³‡æ–™
                if current_analysis["name"] == "é è¨­åˆ†æ":
                    all_raw_data.extend(current_analysis["raw_data"])
                    source_metadata.append({
                        "name": "é è¨­åˆ†æ",
                        "queries": len(current_analysis["raw_data"]),
                        "templates": len(current_analysis["summary_data"])
                    })
                continue
            
            # è¼‰å…¥åˆ†ææª”æ¡ˆ
            source_path = DATA_DIR / source_name
            if not source_path.exists() or not (source_path / "raw_data.json").exists():
                continue
            
            try:
                with open(source_path / "raw_data.json", "r", encoding="utf-8") as f:
                    source_raw_data = json.load(f)
                    all_raw_data.extend(source_raw_data)
                
                # è¼‰å…¥å…ƒè³‡æ–™
                try:
                    with open(source_path / "metadata.json", "r", encoding="utf-8") as f:
                        metadata = json.load(f)
                        source_metadata.append({
                            "name": source_name,
                            "queries": metadata.get("total_queries", len(source_raw_data)),
                            "templates": metadata.get("total_templates", 0)
                        })
                except:
                    source_metadata.append({
                        "name": source_name,
                        "queries": len(source_raw_data),
                        "templates": 0
                    })
                    
            except Exception as e:
                print(f"ç„¡æ³•è¼‰å…¥åˆ†ææª”æ¡ˆ {source_name}: {e}")
                continue
        
        if not all_raw_data:
            raise HTTPException(status_code=400, detail="ç„¡æ³•è¼‰å…¥ä»»ä½•æœ‰æ•ˆçš„åˆ†æè³‡æ–™")
        
        # å»ºç«‹åˆä½µçµ±è¨ˆæ‘˜è¦
        merged_summary = create_summary_data(all_raw_data)
        
        # å»ºç«‹åˆä½µç›®éŒ„
        merged_path.mkdir(exist_ok=True)
        
        # å„²å­˜åˆä½µå¾Œçš„åŸå§‹è³‡æ–™
        with open(merged_path / "raw_data.json", "w", encoding="utf-8") as f:
            json.dump(all_raw_data, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜åˆä½µå¾Œçš„çµ±è¨ˆæ‘˜è¦
        with open(merged_path / "summary.json", "w", encoding="utf-8") as f:
            json.dump(merged_summary, f, ensure_ascii=False, indent=2)
        
        # å»ºç«‹åˆä½µå…ƒè³‡æ–™
        merged_metadata = {
            "original_filename": "merged_analysis",
            "upload_time": datetime.now().isoformat(),
            "total_queries": len(all_raw_data),
            "total_templates": len(merged_summary),
            "merge_info": {
                "merged_from": source_files,
                "merge_time": datetime.now().isoformat(),
                "source_details": source_metadata
            }
        }
        
        with open(merged_path / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(merged_metadata, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "message": f"æˆåŠŸåˆä½µ {len(source_files)} å€‹åˆ†ææª”æ¡ˆ",
            "merged_name": merged_name,
            "total_queries": len(all_raw_data),
            "total_templates": len(merged_summary),
            "source_count": len(source_files),
            "source_files": source_files
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆä½µæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

@app.get("/api/raw_sqls/{template_index}")
async def get_raw_sqls(template_index: int):
    """å–å¾—æŒ‡å®šæ¨£æ¿çš„åŸå§‹SQLåˆ—è¡¨"""
    if 0 <= template_index < len(current_analysis["summary_data"]):
        template = current_analysis["summary_data"][template_index]["template"]
        raw_sqls = current_analysis["template_to_raw_dict"].get(template, [])
        return {"raw_sqls": raw_sqls}
    return {"error": "æ¨¡æ¿ç´¢å¼•ç„¡æ•ˆ"}

@app.get("/api/raw_queries")
async def get_raw_queries(
    page: int = Query(1, ge=1),
    size: int = Query(50, ge=1, le=1000),
    search: str = Query("", description="æœå°‹é—œéµå­—"),
    min_time: float = Query(0, ge=0, description="æœ€å°æŸ¥è©¢æ™‚é–“"),
    sql_type: str = Query("", description="SQLé¡å‹ç¯©é¸"),
    user_filter: str = Query("", description="ç”¨æˆ¶ç¯©é¸"),
    table_filter: str = Query("", description="è¡¨æ ¼ç¯©é¸")
):
    """å–å¾—åŸå§‹æŸ¥è©¢åˆ—è¡¨ï¼Œæ”¯æ´åˆ†é å’Œç¯©é¸"""
    
    # ç¯©é¸è³‡æ–™
    filtered_data = []
    for item in current_analysis["raw_data"]:
        if not item.get("sql"):
            continue
            
        # æŸ¥è©¢æ™‚é–“ç¯©é¸
        if item.get("query_time", 0) < min_time:
            continue
            
        # SQLé¡å‹ç¯©é¸
        if sql_type:
            if get_sql_type(item["sql"]) != sql_type:
                continue
                
        # ç”¨æˆ¶ç¯©é¸
        if user_filter and user_filter.lower() not in (item.get("user", "").lower()):
            continue
            
        # è¡¨æ ¼ç¯©é¸
        if table_filter:
            item_tables = item.get("tables_used", [])
            table_filters = [t.strip().lower() for t in table_filter.split(',') if t.strip()]
            if table_filters:
                found = False
                for table_name in item_tables:
                    for filter_table in table_filters:
                        if filter_table in table_name.lower():
                            found = True
                            break
                    if found:
                        break
                if not found:
                    continue
            
        # æœå°‹é—œéµå­—ç¯©é¸
        if search and search.lower() not in item.get("sql", "").lower():
            continue
            
        filtered_data.append(item)
    
    # æ’åºï¼ˆä¾æŸ¥è©¢æ™‚é–“é™åºï¼‰
    filtered_data.sort(key=lambda x: x.get("query_time", 0), reverse=True)
    
    # åˆ†é 
    total = len(filtered_data)
    start = (page - 1) * size
    end = start + size
    page_data = filtered_data[start:end]
    
    # æ ¼å¼åŒ–è³‡æ–™
    formatted_data = []
    for item in page_data:
        formatted_data.append({
            "sql": item["sql"],
            "query_time": item.get("query_time", 0),
            "lock_time": item.get("lock_time", 0),
            "rows_examined": item.get("rows_examined", 0),
            "rows_sent": item.get("rows_sent", 0),
            "user": item.get("user", ""),
            "host": item.get("host", ""),
            "time": item.get("time", ""),
            "schema": item.get("schema", ""),
            "thread_id": item.get("thread_id", 0),
            "sql_type": get_sql_type(item["sql"]),
            "tables_used": item.get("tables_used", [])
        })
    
    return {
        "data": formatted_data,
        "total": total,
        "page": page,
        "size": size,
        "total_pages": (total + size - 1) // size
    }

@app.get("/api/performance_stats")
async def get_performance_stats():
    """å–å¾—æ•ˆèƒ½åˆ†æçµ±è¨ˆè³‡æ–™"""
    
    # åŸºæœ¬çµ±è¨ˆ
    query_times = [item.get("query_time", 0) for item in current_analysis["raw_data"] if item.get("query_time")]
    
    if not query_times:
        return {"error": "ç„¡æŸ¥è©¢æ™‚é–“è³‡æ–™"}
    
    # SQLé¡å‹çµ±è¨ˆ
    type_stats = Counter()
    time_by_type = defaultdict(list)
    
    for item in current_analysis["raw_data"]:
        if item.get("sql"):
            sql_type = get_sql_type(item["sql"])
            type_stats[sql_type] += 1
            if item.get("query_time"):
                time_by_type[sql_type].append(item["query_time"])
    
    # æ™‚é–“åˆ†å¸ƒçµ±è¨ˆ
    time_ranges = {
        "0-1s": 0,
        "1-5s": 0, 
        "5-10s": 0,
        "10-30s": 0,
        "30s+": 0
    }
    
    for qt in query_times:
        if qt < 1:
            time_ranges["0-1s"] += 1
        elif qt < 5:
            time_ranges["1-5s"] += 1
        elif qt < 10:
            time_ranges["5-10s"] += 1
        elif qt < 30:
            time_ranges["10-30s"] += 1
        else:
            time_ranges["30s+"] += 1
    
    # ç”¨æˆ¶çµ±è¨ˆ
    user_stats = Counter()
    for item in current_analysis["raw_data"]:
        if item.get("user"):
            user_stats[item["user"]] += 1
    
    # è¡¨æ ¼ä½¿ç”¨çµ±è¨ˆ
    table_stats = Counter()
    for item in current_analysis["raw_data"]:
        for table in item.get("tables_used", []):
            table_stats[table] += 1
    
    return {
        "basic_stats": {
            "total_queries": len(current_analysis["raw_data"]),
            "avg_query_time": round(statistics.mean(query_times), 4),
            "median_query_time": round(statistics.median(query_times), 4),
            "max_query_time": round(max(query_times), 4),
            "min_query_time": round(min(query_times), 4)
        },
        "type_stats": dict(type_stats.most_common()),
        "time_ranges": time_ranges,
        "user_stats": dict(user_stats.most_common(10)),
        "table_stats": dict(table_stats.most_common(20)),
        "type_performance": {
            sql_type: {
                "count": len(times),
                "avg_time": round(statistics.mean(times), 4),
                "max_time": round(max(times), 4)
            } for sql_type, times in time_by_type.items() if times
        }
    }

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ å•Ÿå‹• SQL æ…¢æŸ¥è©¢åˆ†æ Dashboard...")
    print("ğŸ“Š æœå‹™å™¨åœ°å€: http://localhost:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)
