"""
åˆ†æè³‡æ–™ç®¡ç†å™¨
"""

import json
import shutil
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
from models.schemas import QueryEntry, SummaryEntry, AnalysisMetadata, CurrentAnalysis
from core.log_parser import LogParser
from core.sql_analyzer import SQLAnalyzer


class DataManager:
    """åˆ†æè³‡æ–™ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "analysis_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.log_parser = LogParser()
        self.sql_analyzer = SQLAnalyzer()
        self.current_analysis = CurrentAnalysis(
            name="é è¨­åˆ†æ",
            summary_data=[],
            template_to_raw_dict={},
            raw_data=[]
        )
    
    def load_analysis_data(self, analysis_name: str = "é è¨­åˆ†æ") -> CurrentAnalysis:
        """
        è¼‰å…¥æŒ‡å®šçš„åˆ†æè³‡æ–™
        
        Args:
            analysis_name: åˆ†ææª”æ¡ˆåç¨±
            
        Returns:
            CurrentAnalysis: ç•¶å‰åˆ†æè³‡æ–™
        """
        if analysis_name == "é è¨­åˆ†æ":
            # è¼‰å…¥é è¨­è³‡æ–™
            try:
                with open("normalized_sql_summary.json", "r", encoding="utf-8") as f:
                    summary_dict_list = json.load(f)
                with open("parsed_slow_log.json", "r", encoding="utf-8") as f:
                    raw_dict_list = json.load(f)
                    
                # è½‰æ›ç‚ºè³‡æ–™é¡åˆ¥
                summary_data = [
                    SummaryEntry(**item) for item in summary_dict_list
                ]
                raw_data = [
                    QueryEntry(**item) for item in raw_dict_list
                ]
            except FileNotFoundError:
                summary_data = []
                raw_data = []
        else:
            # è¼‰å…¥æŒ‡å®šåˆ†ææª”æ¡ˆ
            analysis_path = self.data_dir / analysis_name
            if not analysis_path.exists():
                raise FileNotFoundError(f"åˆ†ææª”æ¡ˆä¸å­˜åœ¨: {analysis_name}")
                
            with open(analysis_path / "summary.json", "r", encoding="utf-8") as f:
                summary_dict_list = json.load(f)
            with open(analysis_path / "raw_data.json", "r", encoding="utf-8") as f:
                raw_dict_list = json.load(f)
                
            # è½‰æ›ç‚ºè³‡æ–™é¡åˆ¥
            summary_data = [
                SummaryEntry(**item) for item in summary_dict_list
            ]
            raw_data = [
                QueryEntry(**item) for item in raw_dict_list
            ]
        
        # å»ºç«‹æ¨£æ¿å°æ‡‰é—œä¿‚
        template_to_raw_dict = self.sql_analyzer.build_template_to_raw_mapping(raw_data)
        
        # æ›´æ–°ç•¶å‰åˆ†æ
        self.current_analysis = CurrentAnalysis(
            name=analysis_name,
            summary_data=summary_data,
            template_to_raw_dict=template_to_raw_dict,
            raw_data=raw_data
        )
        
        return self.current_analysis
    
    def save_analysis(self, analysis_name: str, log_content: str, original_filename: str) -> Dict[str, Any]:
        """
        å„²å­˜æ–°çš„åˆ†æè³‡æ–™
        
        Args:
            analysis_name: åˆ†ææª”æ¡ˆåç¨±
            log_content: LOG æª”æ¡ˆå…§å®¹
            original_filename: åŸå§‹æª”æ¡ˆåç¨±
            
        Returns:
            Dict[str, Any]: å„²å­˜çµæœè³‡è¨Š
        """
        # å»ºç«‹åˆ†æç›®éŒ„
        analysis_path = self.data_dir / analysis_name
        analysis_path.mkdir(exist_ok=True)
        
        try:
            # å„²å­˜åŸå§‹æª”æ¡ˆ
            log_file_path = analysis_path / f"original_{original_filename}"
            with open(log_file_path, "w", encoding="utf-8") as f:
                f.write(log_content)
            
            # è§£æ LOG
            raw_data = self.log_parser.parse_slow_log(log_content)
            
            # å»ºç«‹çµ±è¨ˆæ‘˜è¦
            summary_data = self.sql_analyzer.create_summary_data(raw_data)
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼ä»¥å„²å­˜ JSON
            raw_data_dict = [self._query_entry_to_dict(item) for item in raw_data]
            summary_data_dict = [self._summary_entry_to_dict(item) for item in summary_data]
            
            # å„²å­˜åˆ†æçµæœ
            with open(analysis_path / "raw_data.json", "w", encoding="utf-8") as f:
                json.dump(raw_data_dict, f, ensure_ascii=False, indent=2)
            
            with open(analysis_path / "summary.json", "w", encoding="utf-8") as f:
                json.dump(summary_data_dict, f, ensure_ascii=False, indent=2)
            
            # å„²å­˜å…ƒè³‡æ–™
            metadata = AnalysisMetadata(
                original_filename=original_filename,
                upload_time=datetime.now().isoformat(),
                total_queries=len(raw_data),
                total_templates=len(summary_data)
            )
            
            with open(analysis_path / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(self._metadata_to_dict(metadata), f, ensure_ascii=False, indent=2)
            
            return {
                "success": True,
                "message": f"LOGæª”æ¡ˆ '{original_filename}' ä¸Šå‚³ä¸¦åˆ†æå®Œæˆ",
                "analysis_name": analysis_name,
                "total_queries": len(raw_data),
                "total_templates": len(summary_data)
            }
            
        except Exception as e:
            # æ¸…ç†å¤±æ•—çš„ç›®éŒ„
            if analysis_path.exists():
                shutil.rmtree(analysis_path)
            raise e
    
    def delete_analysis(self, analysis_name: str) -> Dict[str, Any]:
        """
        åˆªé™¤æŒ‡å®šçš„åˆ†ææª”æ¡ˆ
        
        Args:
            analysis_name: åˆ†ææª”æ¡ˆåç¨±
            
        Returns:
            Dict[str, Any]: åˆªé™¤çµæœè³‡è¨Š
        """
        if analysis_name == "é è¨­åˆ†æ":
            raise ValueError("ç„¡æ³•åˆªé™¤é è¨­åˆ†æ")
        
        analysis_path = self.data_dir / analysis_name
        if not analysis_path.exists():
            raise FileNotFoundError("åˆ†ææª”æ¡ˆä¸å­˜åœ¨")
        
        shutil.rmtree(analysis_path)
        
        # å¦‚æœåˆªé™¤çš„æ˜¯ç•¶å‰åˆ†æï¼Œåˆ‡æ›å›é è¨­
        if self.current_analysis.name == analysis_name:
            self.load_analysis_data("é è¨­åˆ†æ")
        
        return {
            "success": True,
            "message": f"åˆ†ææª”æ¡ˆ '{analysis_name}' å·²åˆªé™¤",
            "current_analysis": self.current_analysis.name
        }
    
    def get_analysis_files(self) -> Dict[str, Any]:
        """ç²å–æ‰€æœ‰åˆ†ææª”æ¡ˆåˆ—è¡¨"""
        analysis_files = []
        
        # æª¢æŸ¥åˆ†æç›®éŒ„ä¸­çš„åˆ†ææª”æ¡ˆ
        if self.data_dir.exists():
            for analysis_path in self.data_dir.iterdir():
                if analysis_path.is_dir() and (analysis_path / "summary.json").exists():
                    try:
                        metadata = self._load_metadata(analysis_path.name)
                        analysis_files.append({
                            "name": analysis_path.name,
                            "is_current": analysis_path.name == self.current_analysis.name,
                            "metadata": metadata
                        })
                    except Exception as e:
                        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ {analysis_path.name} çš„è³‡è¨Š: {e}")
                
        return {
            "analysis_files": sorted(analysis_files, key=lambda x: x["metadata"].get("upload_time", ""), reverse=True)
        }

    def _load_metadata(self, analysis_name: str) -> Dict[str, Any]:
        """è¼‰å…¥åˆ†ææª”æ¡ˆçš„å…ƒè³‡æ–™"""
        metadata_file = self.data_dir / analysis_name / "metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥å…ƒè³‡æ–™å¤±æ•—: {e}")
        
        # è¿”å›é è¨­å…ƒè³‡æ–™
        return {
            "total_queries": 0,
            "total_templates": 0,
            "upload_time": "æœªçŸ¥"
        }

    def get_current_analysis_info(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰åˆ†æçš„åŸºæœ¬è³‡è¨Š"""
        # ç§»é™¤è‡ªå‹•è¼‰å…¥é‚è¼¯ï¼Œé¿å…åœ¨åˆ‡æ›åˆ†ææª”æ¡ˆå¾Œåˆè¢«è‡ªå‹•åˆ‡æ›å›ç¬¬ä¸€å€‹æª”æ¡ˆ
        # if not self.current_analysis.summary_data and len(self.current_analysis.raw_data) == 0:
        #     # å¦‚æœç•¶å‰æ²’æœ‰è³‡æ–™ï¼Œå˜—è©¦è¼‰å…¥ç¬¬ä¸€å€‹å¯ç”¨çš„åˆ†ææª”æ¡ˆ
        #     files_data = self.get_analysis_files()
        #     if files_data["analysis_files"]:
        #         try:
        #             first_analysis = files_data["analysis_files"][0]["name"]
        #             self.load_analysis_data(first_analysis)
        #             print(f"ğŸ”„ è‡ªå‹•è¼‰å…¥åˆ†ææª”æ¡ˆ: {first_analysis}")
        #         except Exception as e:
        #             print(f"âš ï¸ è‡ªå‹•è¼‰å…¥å¤±æ•—: {e}")

        metadata = self._load_metadata(self.current_analysis.name)
        return {
            "name": self.current_analysis.name,
            "total_queries": metadata.get("total_queries", 0),
            "total_templates": metadata.get("total_templates", 0),
            "upload_time": metadata.get("upload_time", "æœªçŸ¥")
        }

    def get_template_data(self) -> List[Dict[str, Any]]:
        """ç²å–æ¨£æ¿çµ±è¨ˆè³‡æ–™ï¼ˆç”¨æ–¼æ¨¡æ¿ï¼‰"""
        template_data = []
        for item in self.current_analysis.summary_data:
            template_data.append({
                "template": item.template,
                "type": item.type,
                "count": item.count,
                "avg_query_time": item.avg_query_time,
                "tables_used": item.tables_used
            })
        return template_data

    def get_basic_stats(self) -> Dict[str, Any]:
        """ç²å–åŸºæœ¬çµ±è¨ˆè³‡è¨Š"""
        if not self.current_analysis.raw_data:
            return {
                "total_queries": 0,
                "avg_time": 0.0,
                "max_time": 0.0,
                "median_time": 0.0
            }

        # éæ¿¾æ‰ None å€¼
        query_times = [item.query_time for item in self.current_analysis.raw_data if item.query_time is not None]
        if not query_times:
            return {
                "total_queries": len(self.current_analysis.raw_data),
                "avg_time": 0.0,
                "max_time": 0.0,
                "median_time": 0.0
            }

        query_times.sort()
        
        return {
            "total_queries": len(self.current_analysis.raw_data),
            "avg_time": sum(query_times) / len(query_times),
            "max_time": max(query_times),
            "median_time": query_times[len(query_times) // 2]
        }
    
    def merge_analysis(self, merged_name: str, source_files: List[str]) -> Dict[str, Any]:
        """
        åˆä½µå¤šå€‹åˆ†ææª”æ¡ˆ
        
        Args:
            merged_name: åˆä½µå¾Œçš„æª”æ¡ˆåç¨±
            source_files: ä¾†æºæª”æ¡ˆåˆ—è¡¨
            
        Returns:
            Dict[str, Any]: åˆä½µçµæœè³‡è¨Š
        """
        if len(source_files) < 2:
            raise ValueError("è‡³å°‘éœ€è¦é¸æ“‡ 2 å€‹æª”æ¡ˆé€²è¡Œåˆä½µ")
        
        # æª¢æŸ¥åˆä½µæª”æ¡ˆåç¨±æ˜¯å¦å·²å­˜åœ¨
        merged_path = self.data_dir / merged_name
        if merged_path.exists():
            raise ValueError("åˆä½µæª”æ¡ˆåç¨±å·²å­˜åœ¨ï¼Œè«‹é¸æ“‡å…¶ä»–åç¨±")
        
        # æ”¶é›†æ‰€æœ‰åŸå§‹è³‡æ–™
        all_raw_data = []
        source_metadata = []
        
        for source_name in source_files:
            if source_name == "é è¨­åˆ†æ":
                if self.current_analysis.name == "é è¨­åˆ†æ":
                    all_raw_data.extend(self.current_analysis.raw_data)
                    source_metadata.append({
                        "name": "é è¨­åˆ†æ",
                        "queries": len(self.current_analysis.raw_data),
                        "templates": len(self.current_analysis.summary_data)
                    })
                continue
            
            # è¼‰å…¥åˆ†ææª”æ¡ˆ
            source_path = self.data_dir / source_name
            if not source_path.exists() or not (source_path / "raw_data.json").exists():
                continue
            
            try:
                with open(source_path / "raw_data.json", "r", encoding="utf-8") as f:
                    source_raw_dict = json.load(f)
                    source_raw_data = [QueryEntry(**item) for item in source_raw_dict]
                    all_raw_data.extend(source_raw_data)
                
                # è¼‰å…¥å…ƒè³‡æ–™
                try:
                    with open(source_path / "metadata.json", "r", encoding="utf-8") as f:
                        metadata = json.load(f)
                        source_metadata.append({
                            "name": source_name,
                            "queries": metadata.get("total_queries", len(source_raw_data)),
                            "templates": metadata.get("total_templates", 0)
                        })
                except:
                    source_metadata.append({
                        "name": source_name,
                        "queries": len(source_raw_data),
                        "templates": 0
                    })
                    
            except Exception as e:
                print(f"ç„¡æ³•è¼‰å…¥åˆ†ææª”æ¡ˆ {source_name}: {e}")
                continue
        
        if not all_raw_data:
            raise ValueError("ç„¡æ³•è¼‰å…¥ä»»ä½•æœ‰æ•ˆçš„åˆ†æè³‡æ–™")
        
        # å»ºç«‹åˆä½µçµ±è¨ˆæ‘˜è¦
        merged_summary = self.sql_analyzer.create_summary_data(all_raw_data)
        
        # å»ºç«‹åˆä½µç›®éŒ„
        merged_path.mkdir(exist_ok=True)
        
        # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
        raw_data_dict = [self._query_entry_to_dict(item) for item in all_raw_data]
        summary_data_dict = [self._summary_entry_to_dict(item) for item in merged_summary]
        
        # å„²å­˜åˆä½µå¾Œçš„è³‡æ–™
        with open(merged_path / "raw_data.json", "w", encoding="utf-8") as f:
            json.dump(raw_data_dict, f, ensure_ascii=False, indent=2)
        
        with open(merged_path / "summary.json", "w", encoding="utf-8") as f:
            json.dump(summary_data_dict, f, ensure_ascii=False, indent=2)
        
        # å»ºç«‹åˆä½µå…ƒè³‡æ–™
        merged_metadata = {
            "original_filename": "merged_analysis",
            "upload_time": datetime.now().isoformat(),
            "total_queries": len(all_raw_data),
            "total_templates": len(merged_summary),
            "merge_info": {
                "merged_from": source_files,
                "merge_time": datetime.now().isoformat(),
                "source_details": source_metadata
            }
        }
        
        with open(merged_path / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(merged_metadata, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "message": f"æˆåŠŸåˆä½µ {len(source_files)} å€‹åˆ†ææª”æ¡ˆ",
            "merged_name": merged_name,
            "total_queries": len(all_raw_data),
            "total_templates": len(merged_summary),
            "source_count": len(source_files),
            "source_files": source_files
        }
    
    @staticmethod
    def _query_entry_to_dict(entry: QueryEntry) -> Dict[str, Any]:
        """å°‡ QueryEntry è½‰æ›ç‚ºå­—å…¸"""
        return {
            "time": entry.time,
            "user": entry.user,
            "host": entry.host,
            "thread_id": entry.thread_id,
            "schema": entry.schema,
            "qc_hit": entry.qc_hit,
            "query_time": entry.query_time,
            "lock_time": entry.lock_time,
            "rows_sent": entry.rows_sent,
            "rows_examined": entry.rows_examined,
            "rows_affected": entry.rows_affected,
            "bytes_sent": entry.bytes_sent,
            "timestamp": entry.timestamp,
            "sql": entry.sql,
            "tables_used": entry.tables_used
        }
    
    @staticmethod
    def _summary_entry_to_dict(entry: SummaryEntry) -> Dict[str, Any]:
        """å°‡ SummaryEntry è½‰æ›ç‚ºå­—å…¸"""
        return {
            "template": entry.template,
            "type": entry.type,
            "count": entry.count,
            "avg_query_time": entry.avg_query_time,
            "tables_used": entry.tables_used
        }
    
    @staticmethod
    def _metadata_to_dict(metadata: AnalysisMetadata) -> Dict[str, Any]:
        """å°‡ AnalysisMetadata è½‰æ›ç‚ºå­—å…¸"""
        return {
            "original_filename": metadata.original_filename,
            "upload_time": metadata.upload_time,
            "total_queries": metadata.total_queries,
            "total_templates": metadata.total_templates,
            "merge_info": metadata.merge_info
        } 